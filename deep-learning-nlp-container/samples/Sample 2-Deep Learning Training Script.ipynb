{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n",
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "#Generic Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import re\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "#Keras Packages\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras import utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation\n",
    "from keras import metrics\n",
    "from sklearn import preprocessing\n",
    "from keras import backend\n",
    "\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)\n",
    "\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "\n",
    "#NLTK Packages\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#Spliting Package\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from keras.models import model_from_json\n",
    "from keras.models import model_from_yaml\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"..\\\\kaggle_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regex for any data cleansing activities\n",
    "def RegexRemoval(data):\n",
    "    removed_spc=[]\n",
    "    for i in data['Content']:\n",
    "        removal_spc=re.sub('[^a-zA-Z]',' ',i)\n",
    "        removal_spc=re.sub(r'\\.+','.', removal_spc)\n",
    "        removed_spc.append(removal_spc)\n",
    "    return removed_spc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineering steps like stemming,lemmatization,tokenization can be handled below\n",
    "def cleanData(text, lowercase = True, remove_stops = True, stemming = False, lemmatization = False):\n",
    "        \n",
    "    txt = str(text)\n",
    "    if lowercase:\n",
    "        txt = \" \".join([w.lower() for w in txt.split()])\n",
    "\n",
    "    if remove_stops:\n",
    "        txt = \" \".join([w for w in txt.split() if w not in stop])\n",
    "        \n",
    "    if stemming:\n",
    "        st = PorterStemmer() #choose different stemmers like lancaster for testing activities\n",
    "        txt = \" \".join([st.stem(w) for w in txt.split()])\n",
    "\n",
    "    if lemmatization:\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        txt = \" \".join([wordnet_lemmatizer.lemmatize(w, pos='v') for w in txt.split()])\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLSplit(data):\n",
    "    X = data.content\n",
    "    y = data.result\n",
    "    print(type(X))\n",
    "    print(type(y))\n",
    "    size = 0.1\n",
    "\n",
    "    #Stratified shuffle Split is used. Please use random split(if required)\n",
    "    dataSplit = StratifiedShuffleSplit(n_splits=5, test_size=size, random_state=0)\n",
    "    for train_index, validation_index in dataSplit.split(X,y):\n",
    "        X_train, X_validation = X[train_index], X[validation_index]\n",
    "        y_train, y_validation = y[train_index], y[validation_index]\n",
    "\n",
    "    X_train = X_train[:]\n",
    "    X_validation = X_validation[:]\n",
    "    print(type(X_train))\n",
    "    print(type(y_train))\n",
    "\n",
    "    trainData = pd.concat([X_train,y_train],axis=1)\n",
    "    validateData = pd.concat([X_validation,y_validation],axis=1)\n",
    "    print(\"Train Data Features:\",trainData.shape)\n",
    "    print(\"Validation Data Features:\",validateData.shape)\n",
    "    \n",
    "    return X_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DLTrain(data):\n",
    "    \n",
    "    #RegEx to remove the alpha numerical data\n",
    "    cleanedData=RegexRemoval(data)\n",
    "    cleanedData = pd.DataFrame(cleanedData)\n",
    "    cleanedData.rename(columns={0:'content'},inplace=True)\n",
    "    \n",
    "    data = pd.concat([data,cleanedData],axis=1)\n",
    "       \n",
    "    #Pre-Procesed Data Frame\n",
    "    data = data[['content','Result']]\n",
    "    data.rename(columns={'Result':'result'},inplace=True)\n",
    "    print(data.head())\n",
    "    \n",
    "    data['content'] = data['content'].map(lambda x: cleanData(x, lowercase=False, remove_stops=True, stemming=False, lemmatization = False))\n",
    "    \n",
    "    X,y = MLSplit(data)\n",
    "   ########################### Hyper Parameter Configurations #####################\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=1000, activation='relu', input_shape=(1000,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=1024, activation='relu', input_shape=(1000,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=9, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc',metrics.categorical_accuracy])\n",
    "    #################################################################################\n",
    "    \n",
    "    #Keras Tokenizer\n",
    "    num_max = 1000\n",
    "    tok = Tokenizer(num_words=num_max)\n",
    "    tok.fit_on_texts(X)\n",
    "    X = tok.texts_to_matrix(X,mode='count')\n",
    "    \n",
    "    #Label Encoder\n",
    "    encoder=preprocessing.LabelEncoder()\n",
    "    encoder.fit(y)\n",
    "    y=encoder.transform(y)\n",
    "    num_classes = np.max(y) + 1\n",
    "    y = utils.to_categorical(y,num_classes)\n",
    "    \n",
    "    #Model Building\n",
    "    model.fit(X, y, epochs=10, batch_size=500,verbose=1,validation_split=0.2)\n",
    "    \n",
    "    model_yaml = model.to_yaml()\n",
    "    model_yaml\n",
    "    \n",
    "    model_json = model.to_json()\n",
    "    model_json\n",
    "    \n",
    "    #saving The Models\n",
    "    with open('..\\\\models\\\\tokenizer.pkl', 'wb') as f:\n",
    "        pickle.dump(tok,f)\n",
    "        \n",
    "    with open(\"..\\\\models\\\\model.yaml\", \"w\") as yaml_file:\n",
    "        yaml_file.write(model_yaml)\n",
    "    \n",
    "    with open(\"..\\\\models\\\\model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "        \n",
    "    model.save_weights(\"..\\\\models\\\\model.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "    with open('..\\\\models\\\\encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(encoder,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  \\\n",
      "0  I was contacted originally on XX XX XXXX via c...   \n",
      "1  Thanks for your response and update regarding ...   \n",
      "2  Dear Consumers Financial Protection Bureau   C...   \n",
      "3  Experian XXXX XXXX XXXX XXXX XXXX XXXX XXXX  X...   \n",
      "4  Doctors Business Bureau for            That de...   \n",
      "\n",
      "                      result  \n",
      "0   Action > Debt collection  \n",
      "1          Action > Mortgage  \n",
      "2      Action > Student loan  \n",
      "3  Action > Credit reporting  \n",
      "4   Action > Debt collection  \n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "Train Data Features: (6823, 2)\n",
      "Validation Data Features: (759, 2)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1025024   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 9)                 9225      \n",
      "=================================================================\n",
      "Total params: 2,035,249\n",
      "Trainable params: 2,035,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5458 samples, validate on 1365 samples\n",
      "Epoch 1/10\n",
      "5458/5458 [==============================] - 4s 662us/step - loss: 1.7663 - acc: 0.3193 - categorical_accuracy: 0.3193 - val_loss: 1.4339 - val_acc: 0.3993 - val_categorical_accuracy: 0.3993\n",
      "Epoch 2/10\n",
      "5458/5458 [==============================] - 3s 581us/step - loss: 1.1646 - acc: 0.4152 - categorical_accuracy: 0.4152 - val_loss: 1.1113 - val_acc: 0.5392 - val_categorical_accuracy: 0.5392\n",
      "Epoch 3/10\n",
      "5458/5458 [==============================] - 3s 574us/step - loss: 0.8818 - acc: 0.6755 - categorical_accuracy: 0.6755 - val_loss: 0.7259 - val_acc: 0.7905 - val_categorical_accuracy: 0.7905\n",
      "Epoch 4/10\n",
      "5458/5458 [==============================] - 3s 587us/step - loss: 0.5889 - acc: 0.8074 - categorical_accuracy: 0.8074 - val_loss: 0.7006 - val_acc: 0.7985 - val_categorical_accuracy: 0.7985\n",
      "Epoch 5/10\n",
      "5458/5458 [==============================] - 3s 614us/step - loss: 0.4462 - acc: 0.8540 - categorical_accuracy: 0.8540 - val_loss: 0.6083 - val_acc: 0.8278 - val_categorical_accuracy: 0.8278\n",
      "Epoch 6/10\n",
      "5458/5458 [==============================] - 3s 574us/step - loss: 0.3162 - acc: 0.8943 - categorical_accuracy: 0.8943 - val_loss: 1.0469 - val_acc: 0.7297 - val_categorical_accuracy: 0.7297\n",
      "Epoch 7/10\n",
      "5458/5458 [==============================] - 3s 572us/step - loss: 0.2926 - acc: 0.8992 - categorical_accuracy: 0.8992 - val_loss: 0.7973 - val_acc: 0.8059 - val_categorical_accuracy: 0.8059\n",
      "Epoch 8/10\n",
      "5458/5458 [==============================] - 3s 573us/step - loss: 0.2522 - acc: 0.9159 - categorical_accuracy: 0.9159 - val_loss: 0.6564 - val_acc: 0.8264 - val_categorical_accuracy: 0.8264\n",
      "Epoch 9/10\n",
      "5458/5458 [==============================] - 3s 570us/step - loss: 0.1668 - acc: 0.9460 - categorical_accuracy: 0.9460 - val_loss: 0.7052 - val_acc: 0.8344 - val_categorical_accuracy: 0.8344\n",
      "Epoch 10/10\n",
      "5458/5458 [==============================] - 3s 557us/step - loss: 0.1135 - acc: 0.9654 - categorical_accuracy: 0.9654 - val_loss: 0.9879 - val_acc: 0.7832 - val_categorical_accuracy: 0.7832\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "DLTrain(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(text):\n",
    "\n",
    "    #Loading Tokenizer\n",
    "    with open('..\\\\models\\\\tokenizer.pkl', 'rb') as f:\n",
    "        tok = pickle.load(f)\n",
    "\n",
    "    #loading hyperparameter\n",
    "    with open(\"..\\\\models\\\\model.yaml\", \"r\") as yaml_file:\n",
    "        model_yaml = yaml_file.read()\n",
    "\n",
    "    #loading model\n",
    "    model = model_from_yaml(model_yaml)\n",
    "    model.load_weights(\"..\\\\models\\\\model.h5\")\n",
    "\n",
    "    #loading encoder for converting encoded labels to actual labels\n",
    "    with open('..\\\\models\\\\encoder.pkl', 'rb') as f:\n",
    "        encoder = pickle.load(f)\n",
    "\n",
    "    #inline text\n",
    "    testdataL = text\n",
    "\n",
    "\n",
    "\n",
    "    #tokenization of inline text\n",
    "    X_test=tok.texts_to_matrix(testdataL,mode='count')\n",
    "\n",
    "    #predicting for inline text\n",
    "    prediction = model.predict(np.array(X_test))\n",
    "\n",
    "    #exracting the labels for the predicted text\n",
    "    text_labels = encoder.classes_\n",
    "    predicted_label = text_labels[np.argmax(prediction)]\n",
    "    print(\"predicted category -->\",predicted_label)\n",
    "\n",
    "    prediction_prob = model.predict_proba(np.array([X_test[0]]))\n",
    "    confidence = prediction_prob[0][np.argmax(prediction_prob)]\n",
    "    print(\"confidence score -->\",str(round(confidence,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted category --> Action > Student loan\n",
      "confidence score --> 0.56\n"
     ]
    }
   ],
   "source": [
    "text = [\"I want a student loan\"]\n",
    "validation(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
