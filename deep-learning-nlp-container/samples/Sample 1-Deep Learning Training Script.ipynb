{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n",
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "#Generic Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import re\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "#Keras Packages\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras import utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation\n",
    "from keras import metrics\n",
    "from sklearn import preprocessing\n",
    "from keras import backend\n",
    "\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)\n",
    "\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "\n",
    "#NLTK Packages\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#Spliting Package\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from keras.models import model_from_json\n",
    "from keras.models import model_from_yaml\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"..\\\\en_smalltalk.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regex for any data cleansing activities\n",
    "def RegexRemoval(data):\n",
    "    removed_spc=[]\n",
    "    for i in data['Content']:\n",
    "        removal_spc=re.sub('[^a-zA-Z]',' ',i)\n",
    "        removal_spc=re.sub(r'\\.+','.', removal_spc)\n",
    "        removed_spc.append(removal_spc)\n",
    "    return removed_spc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineering steps like stemming,lemmatization,tokenization can be handled below\n",
    "def cleanData(text, lowercase = True, remove_stops = True, stemming = False, lemmatization = False):\n",
    "        \n",
    "    txt = str(text)\n",
    "    if lowercase:\n",
    "        txt = \" \".join([w.lower() for w in txt.split()])\n",
    "\n",
    "    if remove_stops:\n",
    "        txt = \" \".join([w for w in txt.split() if w not in stop])\n",
    "        \n",
    "    if stemming:\n",
    "        st = PorterStemmer() #choose different stemmers like lancaster for testing activities\n",
    "        txt = \" \".join([st.stem(w) for w in txt.split()])\n",
    "\n",
    "    if lemmatization:\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        txt = \" \".join([wordnet_lemmatizer.lemmatize(w, pos='v') for w in txt.split()])\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLSplit(data):\n",
    "    X = data.content\n",
    "    y = data.result\n",
    "    print(type(X))\n",
    "    print(type(y))\n",
    "    size = 0.1\n",
    "\n",
    "    #Stratified shuffle Split is used. Please use random split(if required)\n",
    "    dataSplit = StratifiedShuffleSplit(n_splits=5, test_size=size, random_state=0)\n",
    "    for train_index, validation_index in dataSplit.split(X,y):\n",
    "        X_train, X_validation = X[train_index], X[validation_index]\n",
    "        y_train, y_validation = y[train_index], y[validation_index]\n",
    "\n",
    "    X_train = X_train[:]\n",
    "    X_validation = X_validation[:]\n",
    "    print(type(X_train))\n",
    "    print(type(y_train))\n",
    "\n",
    "    trainData = pd.concat([X_train,y_train],axis=1)\n",
    "    validateData = pd.concat([X_validation,y_validation],axis=1)\n",
    "    print(\"Train Data Features:\",trainData.shape)\n",
    "    print(\"Validation Data Features:\",validateData.shape)\n",
    "    \n",
    "    return X_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DLTrain(data):\n",
    "    \n",
    "    #RegEx to remove the alpha numerical data\n",
    "    cleanedData=RegexRemoval(data)\n",
    "    cleanedData = pd.DataFrame(cleanedData)\n",
    "    cleanedData.rename(columns={0:'content'},inplace=True)\n",
    "    \n",
    "    data = pd.concat([data,cleanedData],axis=1)\n",
    "       \n",
    "    #Pre-Procesed Data Frame\n",
    "    data = data[['content','Result']]\n",
    "    data.rename(columns={'Result':'result'},inplace=True)\n",
    "    print(data.head())\n",
    "    \n",
    "    data['content'] = data['content'].map(lambda x: cleanData(x, lowercase=False, remove_stops=True, stemming=False, lemmatization = False))\n",
    "    \n",
    "    X,y = MLSplit(data)\n",
    "   ########################### Hyper Parameter Configurations #####################\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=1000, activation='relu', input_shape=(1000,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=1024, activation='relu', input_shape=(1000,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=6, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc',metrics.categorical_accuracy])\n",
    "    #################################################################################\n",
    "    \n",
    "    #Keras Tokenizer\n",
    "    num_max = 1000\n",
    "    tok = Tokenizer(num_words=num_max)\n",
    "    tok.fit_on_texts(X)\n",
    "    X = tok.texts_to_matrix(X,mode='count')\n",
    "    \n",
    "    #Label Encoder\n",
    "    encoder=preprocessing.LabelEncoder()\n",
    "    encoder.fit(y)\n",
    "    y=encoder.transform(y)\n",
    "    num_classes = np.max(y) + 1\n",
    "    y = utils.to_categorical(y,num_classes)\n",
    "    \n",
    "    #Model Building\n",
    "    model.fit(X, y, epochs=10, batch_size=500,verbose=1,validation_split=0.2)\n",
    "    \n",
    "    model_yaml = model.to_yaml()\n",
    "    model_yaml\n",
    "    \n",
    "    model_json = model.to_json()\n",
    "    model_json\n",
    "    \n",
    "    #saving The Models\n",
    "    with open('..\\\\models\\\\tokenizer.pkl', 'wb') as f:\n",
    "        pickle.dump(tok,f)\n",
    "        \n",
    "    with open(\"..\\\\models\\\\model.yaml\", \"w\") as yaml_file:\n",
    "        yaml_file.write(model_yaml)\n",
    "    \n",
    "    with open(\"..\\\\models\\\\model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "        \n",
    "    model.save_weights(\"..\\\\models\\\\model.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "    with open('..\\\\models\\\\encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(encoder,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     content             result\n",
      "0      good morning everyone  action > greeting\n",
      "1       have a great morning  action > greeting\n",
      "2  and a good morning to you  action > greeting\n",
      "3        good morning to you  action > greeting\n",
      "4         hello good morning  action > greeting\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "Train Data Features: (1115, 2)\n",
      "Validation Data Features: (124, 2)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1025024   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 6150      \n",
      "=================================================================\n",
      "Total params: 2,032,174\n",
      "Trainable params: 2,032,174\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 892 samples, validate on 223 samples\n",
      "Epoch 1/10\n",
      "892/892 [==============================] - 1s 1ms/step - loss: 1.6885 - acc: 0.3352 - categorical_accuracy: 0.3352 - val_loss: 1.4091 - val_acc: 0.4843 - val_categorical_accuracy: 0.4843\n",
      "Epoch 2/10\n",
      "892/892 [==============================] - 1s 601us/step - loss: 1.3616 - acc: 0.4608 - categorical_accuracy: 0.4608 - val_loss: 1.2040 - val_acc: 0.4843 - val_categorical_accuracy: 0.4843\n",
      "Epoch 3/10\n",
      "892/892 [==============================] - 1s 620us/step - loss: 1.1327 - acc: 0.4608 - categorical_accuracy: 0.4608 - val_loss: 1.0135 - val_acc: 0.4843 - val_categorical_accuracy: 0.4843\n",
      "Epoch 4/10\n",
      "892/892 [==============================] - 1s 627us/step - loss: 0.9499 - acc: 0.4742 - categorical_accuracy: 0.4742 - val_loss: 0.9251 - val_acc: 0.5740 - val_categorical_accuracy: 0.5740\n",
      "Epoch 5/10\n",
      "892/892 [==============================] - 1s 627us/step - loss: 0.8219 - acc: 0.6413 - categorical_accuracy: 0.6413 - val_loss: 0.7457 - val_acc: 0.7444 - val_categorical_accuracy: 0.7444\n",
      "Epoch 6/10\n",
      "892/892 [==============================] - 1s 622us/step - loss: 0.6050 - acc: 0.8274 - categorical_accuracy: 0.8274 - val_loss: 0.5939 - val_acc: 0.8520 - val_categorical_accuracy: 0.8520\n",
      "Epoch 7/10\n",
      "892/892 [==============================] - 1s 747us/step - loss: 0.4519 - acc: 0.8845 - categorical_accuracy: 0.8845 - val_loss: 0.7626 - val_acc: 0.7309 - val_categorical_accuracy: 0.7309\n",
      "Epoch 8/10\n",
      "892/892 [==============================] - 1s 644us/step - loss: 0.5363 - acc: 0.7915 - categorical_accuracy: 0.7915 - val_loss: 0.4941 - val_acc: 0.8565 - val_categorical_accuracy: 0.8565\n",
      "Epoch 9/10\n",
      "892/892 [==============================] - 1s 584us/step - loss: 0.3071 - acc: 0.9316 - categorical_accuracy: 0.9316 - val_loss: 0.4311 - val_acc: 0.8700 - val_categorical_accuracy: 0.8700\n",
      "Epoch 10/10\n",
      "892/892 [==============================] - 1s 598us/step - loss: 0.2547 - acc: 0.9316 - categorical_accuracy: 0.9316 - val_loss: 0.4261 - val_acc: 0.8655 - val_categorical_accuracy: 0.8655\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "DLTrain(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The below is just for validation purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(text):\n",
    "    \n",
    "    #Loading Tokenizer\n",
    "    with open('..\\\\models\\\\tokenizer.pkl', 'rb') as f:\n",
    "        tok = pickle.load(f)\n",
    "    \n",
    "    #loading hyperparameter\n",
    "    with open(\"..\\\\models\\\\model.yaml\", \"r\") as yaml_file:\n",
    "        model_yaml = yaml_file.read()\n",
    "        \n",
    "    #loading model\n",
    "    model = model_from_yaml(model_yaml)\n",
    "    model.load_weights(\"..\\\\models\\\\model.h5\")\n",
    "    \n",
    "    #loading encoder for converting encoded labels to actual labels\n",
    "    with open('..\\\\models\\\\encoder.pkl', 'rb') as f:\n",
    "        encoder = pickle.load(f)\n",
    "    \n",
    "    #inline text\n",
    "    testdataL = text\n",
    "    \n",
    "    \n",
    "    \n",
    "    #tokenization of inline text\n",
    "    X_test=tok.texts_to_matrix(testdataL,mode='count')\n",
    "    \n",
    "    #predicting for inline text\n",
    "    prediction = model.predict(np.array(X_test))\n",
    "    \n",
    "    #exracting the labels for the predicted text\n",
    "    text_labels = encoder.classes_\n",
    "    predicted_label = text_labels[np.argmax(prediction)]\n",
    "    print(\"predicted category -->\",predicted_label)\n",
    "    \n",
    "    prediction_prob = model.predict_proba(np.array([X_test[0]]))\n",
    "    confidence = prediction_prob[0][np.argmax(prediction_prob)]\n",
    "    print(\"confidence score -->\",str(round(confidence,2)))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted category --> action > escalate\n",
      "confidence score --> 0.74\n"
     ]
    }
   ],
   "source": [
    "text = [\"I want to talk to your manager\"]\n",
    "validation(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
